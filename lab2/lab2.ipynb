{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 classification with a neural network\n",
    "# Author: Alexey Gladyshev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(47)\n",
    "dataset_root_path = '/home/agladyshev/Documents/UNN/DL/Datasets/cifar-10-batches-py'\n",
    "weights_path = os.path.join('./weights', 'Lenet5XXX_weights_51_acc.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load CIFAR-10 dataset and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager:\n",
    "    def __init__(self, dataset_root_path, like_images=False):\n",
    "        def get_data_batch(data_path):\n",
    "            with open(data_path, 'rb') as fo:\n",
    "                raw_train_data = pickle.load(fo, encoding='bytes')\n",
    "            keys = list(raw_train_data.keys())\n",
    "            return np.array(raw_train_data[keys[2]]), np.array(raw_train_data[keys[1]])\n",
    "\n",
    "        train_data_1, train_labels_1 = get_data_batch(os.path.join(dataset_root_path, 'data_batch_1'))\n",
    "        train_data_2, train_labels_2 = get_data_batch(os.path.join(dataset_root_path, 'data_batch_2'))\n",
    "        train_data_3, train_labels_3 = get_data_batch(os.path.join(dataset_root_path, 'data_batch_3'))\n",
    "        train_data_4, train_labels_4 = get_data_batch(os.path.join(dataset_root_path, 'data_batch_4'))\n",
    "        train_data_5, train_labels_5 = get_data_batch(os.path.join(dataset_root_path, 'data_batch_5'))\n",
    "        self.test_data, self.test_labels = get_data_batch(os.path.join(dataset_root_path, 'test_batch'))\n",
    "\n",
    "        self.train_data = np.concatenate((train_data_1, train_data_2, train_data_3, train_data_4, train_data_5), axis=0)\n",
    "        self.train_labels = np.concatenate((train_labels_1, train_labels_2, train_labels_3, train_labels_4, train_labels_5), axis=0)\n",
    "\n",
    "        self.train_data = (self.train_data - np.mean(self.train_data, axis=0)) / np.std(self.train_data, axis=0)\n",
    "        self.test_data = (self.test_data - np.mean(self.test_data, axis=0)) / np.std(self.test_data, axis=0)\n",
    "        if like_images:\n",
    "            self.train_data = self.train_data.reshape((len(self.train_data), 3, 32, 32))\n",
    "            self.test_data = self.test_data.reshape((len(self.test_data), 3, 32, 32))\n",
    "\n",
    "        self.train_data, self.val_data, self.train_labels, self.val_labels = train_test_split(self.train_data,\n",
    "                                                                                              self.train_labels,\n",
    "                                                                                              test_size=0.1,\n",
    "                                                                                              random_state=47)\n",
    "\n",
    "    def get_test_batch(self, batch_size):\n",
    "        return self.train_data[:batch_size], self.train_labels[:batch_size]\n",
    "\n",
    "    def get_train_data(self):\n",
    "        return self.train_data, self.val_data, self.train_labels, self.val_labels\n",
    "\n",
    "    def get_test_data(self):\n",
    "        return self.test_data, self.test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_manager = DataManager(dataset_root_path, like_images=True)\n",
    "X_train, X_val, y_train, y_val = data_manager.get_train_data()\n",
    "X_test, y_test = data_manager.get_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NN implementation, layers, activations, loss functions and optimizer\n",
    "\n",
    "В качестве оптимизатора используется `стохастический градиентный спуск`, он реализован в методе `backward()` у тех слоев, где это необходимо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_initialization(shape, in_size, out_size):\n",
    "    return np.random.uniform(-1, 1, size=shape) * np.sqrt(6. / (in_size + out_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.name = 'layer'\n",
    "\n",
    "    def forward(self, input):\n",
    "        pass\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        pass\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.name, None\n",
    "\n",
    "    def set_state(self, params_dict):\n",
    "        pass\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = 'ReLU'\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = np.maximum(0, input)\n",
    "        return output\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        relu_grad = input > 0\n",
    "        return grad_output * relu_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_units, output_units, learning_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.name = 'Dense'\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.weights = xavier_initialization((input_units, output_units), input_units, output_units)\n",
    "        self.biases = np.zeros(output_units)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        input shape: [batch, input_units]\n",
    "        output shape: [batch, output units]\n",
    "        \"\"\"\n",
    "        return np.dot(input, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        grad_input = np.dot(grad_output, self.weights.T)\n",
    "\n",
    "        grad_weights = np.dot(input.T, grad_output)\n",
    "        grad_biases = np.sum(grad_output, axis=0)\n",
    "\n",
    "        self.weights = self.weights - self.learning_rate * grad_weights\n",
    "        self.biases = self.biases - self.learning_rate * grad_biases\n",
    "\n",
    "        return grad_input\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.name, {'weights': self.weights, 'biases': self.biases}\n",
    "\n",
    "    def set_state(self, params_dict):\n",
    "        self.weights = params_dict['weights']\n",
    "        self.biases = params_dict['biases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = 'Flatten'\n",
    "\n",
    "    def forward(self, input):\n",
    "        n_samples, n_channels, height, width = input.shape\n",
    "        return input.reshape((n_samples, n_channels * height * width))\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        return grad_output.reshape(input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(Layer):\n",
    "    def __init__(self, p):\n",
    "        super().__init__()\n",
    "        self.name = 'Dropout'\n",
    "        self.p = p\n",
    "        self.__mask_cache = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.__mask_cache = np.random.binomial(1, self.p, size=input.shape) / self.p\n",
    "        out = input * self.__mask_cache\n",
    "        return out\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        return grad_output * self.__mask_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_average(running, new, gamma=.9):\n",
    "    return gamma * running + (1. - gamma) * new\n",
    "\n",
    "\n",
    "class BatchNormalization(Layer):\n",
    "    def __init__(self, train_mode=True, learning_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.name = 'BatchNormalization'\n",
    "        self.train_mode = train_mode\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.gamma = 1\n",
    "        self.beta = 0\n",
    "\n",
    "        self.__eps = 0.0000001\n",
    "        self.__running_mean = 0\n",
    "        self.__running_var = 1\n",
    "        self.__input_norm = None\n",
    "        self.__batch_mean = None\n",
    "        self.__batch_var = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.__batch_mean = np.mean(input, axis=0)\n",
    "        self.__batch_var = np.var(input, axis=0)\n",
    "        self.__running_mean = running_average(self.__running_mean, self.__batch_mean)\n",
    "        self.__running_var = running_average(self.__running_var, self.__batch_var)\n",
    "\n",
    "        if self.train_mode:\n",
    "            self.__input_norm = (input - self.__batch_mean) / np.sqrt(self.__batch_var + self.__eps)\n",
    "            out = self.gamma * self.__input_norm + self.beta\n",
    "        else:\n",
    "            input_norm = (input - self.__running_mean) / np.sqrt(self.__running_var + self.__eps)\n",
    "            out = self.gamma * input_norm + self.beta\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        n = input.shape[0]\n",
    "        d = input.shape[1]\n",
    "\n",
    "        input_mu = input - self.__batch_mean\n",
    "        std_inv = 1. / np.sqrt(self.__batch_var + self.__eps)\n",
    "\n",
    "        grad_input_norm = grad_output * self.gamma\n",
    "        grad_var = np.sum(grad_input_norm * input_mu, axis=0) * -.5 * std_inv ** 3\n",
    "        grad_mu = np.sum(grad_input_norm * -std_inv, axis=0) + grad_var * np.mean(-2. * input_mu, axis=0)\n",
    "\n",
    "        grad_input = (grad_input_norm * std_inv) + (grad_var * 2 * input_mu / n) + (grad_mu / n)\n",
    "        grad_gamma = np.sum(grad_output * self.__input_norm, axis=0)\n",
    "        grad_beta = np.sum(grad_output, axis=0)\n",
    "\n",
    "        self.gamma = self.gamma - self.learning_rate * grad_gamma\n",
    "        self.beta = self.beta - self.learning_rate * grad_beta\n",
    "\n",
    "        return grad_input\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.name, {'gamma': self.gamma, 'beta': self.beta}\n",
    "\n",
    "    def set_state(self, params_dict):\n",
    "        self.gamma = params_dict['gamma']\n",
    "        self.beta = params_dict['beta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_im2col_indices(x_shape, field_height, field_width, padding=1, stride=1):\n",
    "    N, C, H, W = x_shape\n",
    "    assert (H + 2 * padding - field_height) % stride == 0\n",
    "    assert (W + 2 * padding - field_height) % stride == 0\n",
    "    out_height = int((H + 2 * padding - field_height) / stride + 1)\n",
    "    out_width = int((W + 2 * padding - field_width) / stride + 1)\n",
    "\n",
    "    i0 = np.repeat(np.arange(field_height), field_width)\n",
    "    i0 = np.tile(i0, C)\n",
    "    i1 = stride * np.repeat(np.arange(out_height), out_width)\n",
    "    j0 = np.tile(np.arange(field_width), field_height * C)\n",
    "    j1 = stride * np.tile(np.arange(out_width), out_height)\n",
    "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "\n",
    "    k = np.repeat(np.arange(C), field_height * field_width).reshape(-1, 1)\n",
    "\n",
    "    return k.astype(int), i.astype(int), j.astype(int)\n",
    "\n",
    "\n",
    "def im2col_indices(x, field_height, field_width, padding=1, stride=1):\n",
    "    p = padding\n",
    "    x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n",
    "\n",
    "    k, i, j = get_im2col_indices(x.shape, field_height, field_width, padding, stride)\n",
    "\n",
    "    cols = x_padded[:, k, i, j]\n",
    "    C = x.shape[1]\n",
    "    cols = cols.transpose(1, 2, 0).reshape(field_height * field_width * C, -1)\n",
    "    return cols\n",
    "\n",
    "\n",
    "def col2im_indices(cols, x_shape, field_height=3, field_width=3, padding=1, stride=1):\n",
    "    N, C, H, W = x_shape\n",
    "    H_padded, W_padded = H + 2 * padding, W + 2 * padding\n",
    "    x_padded = np.zeros((N, C, H_padded, W_padded), dtype=cols.dtype)\n",
    "    k, i, j = get_im2col_indices(x_shape, field_height, field_width, padding, stride)\n",
    "    cols_reshaped = cols.reshape(C * field_height * field_width, -1, N)\n",
    "    cols_reshaped = cols_reshaped.transpose(2, 0, 1)\n",
    "    np.add.at(x_padded, (slice(None), k, i, j), cols_reshaped)\n",
    "    if padding == 0:\n",
    "        return x_padded\n",
    "    return x_padded[:, :, padding:-padding, padding:-padding]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(Layer):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, learning_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.name = 'Conv'\n",
    "        self.learning_rate = learning_rate\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        self.filters = xavier_initialization((out_channels, in_channels, kernel_size[0], kernel_size[1]), in_channels, out_channels)\n",
    "        self.biases = np.zeros(out_channels)\n",
    "\n",
    "        self.input_col = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        input shape: [batch, in_channels, in_height, in_width]\n",
    "        output shape: [batch, out_channels, ..., ...]\n",
    "        \"\"\"\n",
    "        n_filters, d_filter, h_filter, w_filter = self.filters.shape\n",
    "        n_x, d_x, h_x, w_x = input.shape\n",
    "        h_out = int((h_x - h_filter + 2 * self.padding) / self.stride) + 1\n",
    "        w_out = int((w_x - w_filter + 2 * self.padding) / self.stride) + 1\n",
    "\n",
    "        self.input_col = im2col_indices(input, h_filter, w_filter, padding=self.padding, stride=self.stride)\n",
    "        filters_col = self.filters.reshape(n_filters, -1)\n",
    "\n",
    "        out = np.dot(filters_col, self.input_col) + self.biases.reshape(-1, 1)\n",
    "        out = out.reshape(n_filters, h_out, w_out, n_x)\n",
    "        out = out.transpose(3, 0, 1, 2)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        n_filters, d_filter, h_filter, w_filter = self.filters.shape\n",
    "\n",
    "        grad_biases = np.sum(grad_output, axis=(0, 2, 3))\n",
    "\n",
    "        grad_output_reshaped = grad_output.transpose(1, 2, 3, 0).reshape(n_filters, -1)\n",
    "        grad_filters = np.dot(grad_output_reshaped, self.input_col.T)\n",
    "        grad_filters = grad_filters.reshape(self.filters.shape)\n",
    "\n",
    "        filters_reshaped = self.filters.reshape(n_filters, -1)\n",
    "        grad_input_col = np.dot(filters_reshaped.T, grad_output_reshaped)\n",
    "        grad_input = col2im_indices(grad_input_col, input.shape, h_filter, w_filter, padding=self.padding, stride=self.stride)\n",
    "\n",
    "        self.filters = self.filters - self.learning_rate * grad_filters\n",
    "        self.biases = self.biases - self.learning_rate * grad_biases\n",
    "\n",
    "        return grad_input\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.name, {'filters': self.filters, 'biases': self.biases}\n",
    "\n",
    "    def set_state(self, params_dict):\n",
    "        self.filters = params_dict['filters']\n",
    "        self.biases = params_dict['biases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoolingLayer(Layer):\n",
    "    def __init__(self, name, size=2, stride=2):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.size = size\n",
    "        self.stride = stride\n",
    "\n",
    "        self.forward_pool_function = None\n",
    "        self.backward_pool_function = None\n",
    "\n",
    "        self.__input_reshaped = None\n",
    "        self.__input_col = None\n",
    "        self.__max_idx_cache = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        n, d, h, w = input.shape\n",
    "        h_out = int((h - self.size) / self.stride) + 1\n",
    "        w_out = int((w - self.size) / self.stride) + 1\n",
    "\n",
    "        self.__input_reshaped = input.reshape(n * d, 1, h, w)\n",
    "        self.__input_col = im2col_indices(self.__input_reshaped, self.size, self.size, padding=0, stride=self.stride)\n",
    "\n",
    "        out, self.__max_idx_cache = self.forward_pool_function(self.__input_col)\n",
    "        out = out.reshape(h_out, w_out, n, d)\n",
    "        out = out.transpose(2, 3, 0, 1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        n, d, w, h = input.shape\n",
    "\n",
    "        grad_input_col = np.zeros_like(self.__input_col)\n",
    "        grad_output_col = grad_output.transpose(2, 3, 0, 1).ravel()\n",
    "\n",
    "        grad_input = self.backward_pool_function(grad_input_col, grad_output_col, self.__max_idx_cache)\n",
    "        grad_input = col2im_indices(grad_input, (n * d, 1, h, w), self.size, self.size, padding=0, stride=self.stride)\n",
    "        grad_input = grad_input.reshape(input.shape)\n",
    "\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_max_pool(input_col):\n",
    "    max_idx = np.argmax(input_col, axis=0)\n",
    "    out = input_col[max_idx, range(max_idx.size)]\n",
    "    return out, max_idx\n",
    "\n",
    "\n",
    "def backward_max_pool(grad_input_col, grad_output_col, max_idx_cache):\n",
    "    grad_input_col[max_idx_cache, range(grad_output_col.size)] = grad_output_col\n",
    "    return grad_input_col\n",
    "\n",
    "\n",
    "class MaxPool(PoolingLayer):\n",
    "    def __init__(self, size=2, stride=2):\n",
    "        super().__init__('MaxPool', size=size, stride=stride)\n",
    "        self.forward_pool_function = forward_max_pool\n",
    "        self.backward_pool_function = backward_max_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxCCE:\n",
    "    @staticmethod\n",
    "    def compute_loss(logits, reference_answers):\n",
    "        logits_for_answers = logits[np.arange(len(logits)), reference_answers]\n",
    "        xentropy = -logits_for_answers + np.log(np.sum(np.exp(logits), axis=-1))\n",
    "        return xentropy\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_grad(logits, reference_answers):\n",
    "        ones_for_answers = np.zeros_like(logits)\n",
    "        ones_for_answers[np.arange(len(logits)), reference_answers] = 1\n",
    "        softmax = np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True)\n",
    "        return (- ones_for_answers + softmax) / logits.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.name = 'BaseNet'\n",
    "        self.learning_rate = 0.1\n",
    "        self.network = None\n",
    "\n",
    "        self.__accuracy = 0.\n",
    "\n",
    "    def train_on_batch(self, X, y, loss_function):\n",
    "        layer_activations = self.__forward(X)\n",
    "        layer_inputs = [X] + layer_activations\n",
    "        logits = layer_activations[-1]\n",
    "\n",
    "        loss = loss_function.compute_loss(logits, y)\n",
    "        loss_grad = loss_function.compute_grad(logits, y)\n",
    "\n",
    "        current_grad = loss_grad\n",
    "        for layer_ind in reversed(range(len(self.network))):\n",
    "            current_grad = self.network[layer_ind].backward(layer_inputs[layer_ind], current_grad)\n",
    "\n",
    "        return np.mean(loss)\n",
    "\n",
    "    def predict(self, X, batch_step=1000):\n",
    "        n_iterations = 1 + int(len(X) / batch_step)\n",
    "        result = np.array([], dtype=int)\n",
    "        for iteration in range(n_iterations):\n",
    "            start_ind = batch_step * iteration\n",
    "            result = np.append(result, self.__predict_on_batch(X[start_ind:min(start_ind + batch_step, len(X)), :]))\n",
    "        return result\n",
    "\n",
    "    def save_state_dict(self, accuracy, save_root_path='.'):\n",
    "        if accuracy >= self.__accuracy:\n",
    "            self.__accuracy = accuracy\n",
    "            model_state = {}\n",
    "            for layer_ind, layer in enumerate(self.network):\n",
    "                layer_name, params_dict = layer.get_state()\n",
    "                model_state[layer_ind] = {'layer_name': layer_name, 'params_dict': params_dict}\n",
    "\n",
    "            with open(os.path.join(save_root_path, '{}_weights.tar'.format(self.name)), 'wb') as handle:\n",
    "                pickle.dump(model_state, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def load_state_dict(self, weights_path):\n",
    "        with open(weights_path, 'rb') as handle:\n",
    "            model_state = pickle.load(handle)\n",
    "        for layer_ind, layer_state_dict in model_state.items():\n",
    "            layer_params = layer_state_dict['params_dict']\n",
    "            if layer_params is not None:\n",
    "                self.network[layer_ind].set_state(layer_params)\n",
    "\n",
    "    def __forward(self, X):\n",
    "        input = X\n",
    "        activations = []\n",
    "        for layer in self.network:\n",
    "            input = layer.forward(input)\n",
    "            activations.append(input)\n",
    "        return activations\n",
    "\n",
    "    def __predict_on_batch(self, X):\n",
    "        logits = self.__forward(X)[-1]\n",
    "        return logits.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoName(Model):\n",
    "    def __init__(self, learning_rate):\n",
    "        super().__init__()\n",
    "        self.name = 'NoName'\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.network = []\n",
    "        self.network.append(Conv(3, 64, (3, 3), learning_rate=learning_rate))\n",
    "        self.network.append(ReLU())\n",
    "        self.network.append(MaxPool(2, 1))\n",
    "        self.network.append(BatchNormalization())\n",
    "\n",
    "        self.network.append(Conv(64, 128, (3, 3), learning_rate=learning_rate))\n",
    "        self.network.append(ReLU())\n",
    "        self.network.append(MaxPool(2, 1))\n",
    "        self.network.append(BatchNormalization())\n",
    "\n",
    "        self.network.append(Conv(128, 256, (3, 3), learning_rate=learning_rate))\n",
    "        self.network.append(ReLU())\n",
    "        self.network.append(MaxPool(2, 1))\n",
    "        self.network.append(BatchNormalization())\n",
    "\n",
    "        self.network.append(Flatten())\n",
    "\n",
    "        self.network.append(Dense(135424, 128))\n",
    "        self.network.append(ReLU())\n",
    "        self.network.append(Dropout(0.7))\n",
    "        self.network.append(BatchNormalization())\n",
    "\n",
    "        self.network.append(Dense(128, 512))\n",
    "        self.network.append(ReLU())\n",
    "        self.network.append(Dropout(0.7))\n",
    "        self.network.append(BatchNormalization())\n",
    "\n",
    "        self.network.append(Dense(512, 1024))\n",
    "        self.network.append(ReLU())\n",
    "        self.network.append(Dropout(0.7))\n",
    "        self.network.append(BatchNormalization())\n",
    "\n",
    "        self.network.append(Dense(1024, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NoName(learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Loss:  3.333077163854842\n",
      "Train accuracy:  0.3\n",
      "Epoch 1\n",
      "Loss:  2.577919292553527\n",
      "Train accuracy:  0.8\n",
      "Epoch 2\n",
      "Loss:  0.6182426293711744\n",
      "Train accuracy:  0.9\n",
      "Epoch 3\n",
      "Loss:  0.5399795269807296\n",
      "Train accuracy:  1.0\n",
      "Epoch 4\n",
      "Loss:  0.22149153222401816\n",
      "Train accuracy:  1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_x_batch, test_y_batch = data_manager.get_test_batch(batch_size=10)\n",
    "\n",
    "train_log = []\n",
    "for epoch in range(5):\n",
    "    loss = network.train_on_batch(test_x_batch, test_y_batch, SoftmaxCCE)\n",
    "    train_log.append(np.mean(network.predict(test_x_batch, batch_step=32) == test_y_batch))\n",
    "    print(\"Epoch\", epoch)\n",
    "    print(\"Loss: \", loss)\n",
    "    print(\"Train accuracy: \", train_log[-1])\n",
    "\n",
    "plt.plot(train_log, label='train accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.load_state_dict(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = network.predict(X_test, batch_step=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.4614\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: ', accuracy_score(pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[371  23  44  31  23  10  12  14  67  25]\n",
      " [ 47 556  28  23  21  15  34  16  73 148]\n",
      " [ 89  13 249  54  81  71  44  41  15  22]\n",
      " [  9  11  49 156  35  82  42  31  18  20]\n",
      " [ 30   7 175  74 355  87  78  91  18   4]\n",
      " [ 29  21 149 327 115 510  57 136  27  24]\n",
      " [ 20  23 135 177 181  90 652  47  13  31]\n",
      " [ 35  22  75  56 118  84  26 518   9  37]\n",
      " [290  91  63  41  48  15  20  19 662 104]\n",
      " [ 80 233  33  61  23  36  35  87  98 585]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results\n",
    "### The best NN architecture\n",
    "\n",
    "\n",
    "### Learning rate strategy and batch size\n",
    "\n",
    "\n",
    "### Data augmentations\n",
    "\n",
    "\n",
    "### The best model test accuracy\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
